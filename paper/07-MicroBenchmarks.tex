\begin{figure*}[t!]%
	\graphicspath{{results/microbenchmarks/}}
	\scriptsize
	\null\hfill
	\begin{subfigure}[t]{0.2in}
		\input{results/microbenchmarks/plot_paper_ylabel_scalar.tex}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{1.3in}
		\input{results/microbenchmarks/plot_paper_encode_16bit_seq.tex}
		\setcounter{subfigure}{0}
		\vspace{-3mm}
		\caption{Encode Scalar}
		\label{fig:codingbenchmark:hardening:seq}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{1.3in}
		\input{results/microbenchmarks/plot_paper_decode_16bit_seq.tex}
		\setcounter{subfigure}{2}
		\vspace{-3mm}
		\caption{Decode Scalar}
		\label{fig:codingbenchmark:decode:seq}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{1.3in}
		\input{results/microbenchmarks/plot_paper_check_16bit_seq.tex}
		\setcounter{subfigure}{4}
		\vspace{-3mm}
		\caption{Detect Scalar}
		\label{fig:codingbenchmark:check:seq}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{1.3in}
		\input{results/microbenchmarks/plot_paper_decode_16bit_seq_inv.tex}
		\setcounter{subfigure}{6}
		\vspace{-3mm}
		\caption{Refined Decode Scalar}
		\label{fig:refinements:decode:seq:zoom}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{1.3in}
		\input{results/microbenchmarks/plot_paper_check_16bit_seq_inv.tex}
		\setcounter{subfigure}{8}
		\vspace{-3mm}
		\caption{Refined Detect Scalar}
		\label{fig:refinements:check:seq:zoom}
	\end{subfigure}
	\hfill\null
	\\
	\null\hfill
	\begin{subfigure}[t]{0.2in}
		\input{results/microbenchmarks/plot_paper_ylabel_vectorized.tex}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{1.3in}
		\input{results/microbenchmarks/plot_paper_encode_16bit_vec.tex}
		\setcounter{subfigure}{1}
		\vspace{-3mm}
		\caption{Encode SSE4.2 / AVX2}
		\label{fig:codingbenchmark:hardening:vec}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{1.3in}
		\input{results/microbenchmarks/plot_paper_decode_16bit_vec.tex}
		\setcounter{subfigure}{3}
		\vspace{-3mm}
		\caption{Decode SSE4.2 / AVX2}
		\label{fig:codingbenchmark:decode:vec}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{1.3in}
		\input{results/microbenchmarks/plot_paper_check_16bit_vec.tex}
		\setcounter{subfigure}{5}
		\vspace{-3mm}
		\caption{Detect SSE4.2 / AVX2}
		\label{fig:codingbenchmark:check:vec}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{1.3in}
		\input{results/microbenchmarks/plot_paper_decode_16bit_vec_inv.tex}
		\setcounter{subfigure}{7}
		\vspace{-3mm}
		\caption{SSE4.2 / AVX2}
		\label{fig:refinements:decode:vec:zoom}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{1.3in}
		\input{results/microbenchmarks/plot_paper_check_16bit_vec_inv.tex}
		\setcounter{subfigure}{9}
		\vspace{-3mm}
		\caption{SSE4.2 / AVX2}
		\label{fig:refinements:check:vec:zoom}
	\end{subfigure}
	\hfill\null
	\\
	\null\hfill
	\input{results/microbenchmarks/plot_paper_legend.tex}
	\hfill\null
	\vspace{-4mm}
	\caption{Micro Benchmarks -- runtimes for scalar, SSE4.2 and AVX2 code functions. For XOR the block size is varied, while for Hamming and AN a loop unroll factor is varied. \(AN_U=\) unsigned, \(AN_S=\) signed.}%
	\label{fig:codingbenchmark:encode}%
	\vspace{-3mm}
\end{figure*}

\section{Mirco Benchmarks}
\label{sec:MicroBenchmarks}

We also conducted micro benchmarks to support our error code decision from a performance perspective. 

\subsection{Error Code Evaluation}

In our first micro benchmark series, we compared AN coding with other well-known und heavily applied coding schemes to show the advantages of AN coding for software-based error detection. In detail, we consider Hamming codes as presented in Section~\ref{sec:ErrorCoding} and checksums, both \emph{systematic error codes} where data bits and additional, redundant bits are separable. \emph{Checksums} add a small\=sized value derived from an arbitrary data block forming a hardened data block allowing limited error detection. Nowadays, the term checksum is also used when hash functions are applied. There exists a multitude of algorithms with varying complexity and hardware support, e.g. parity bits, parity words, Message\=Digest Algorithms (e.g. MD5)~\cite{md5,Sivathanu2005} or cyclic redundancy checks (e.g. CRC32)~\cite{peterson1961cyclic,Sivathanu2005}. In the case of parity bits (words), the data bits (words) are summed up using the binary exclusive or operation (XOR, \(\oplus\)). The size of the resulting checksum can be arbitrary, but is usually either a single bit or aligned to machine words, respectively, for the sake of performance. In the following, we restrict our discussion to XOR checksums, since they are one of the most simple types of checksums. Furthermore, both Hamming code and XOR checksum can be vectorized~\cite{mula2016avx2popcount,warren2013hacker} which is important for performance. 

In detail, we compared runtime overheads for the following functions: \emph{hardening}, \emph{decoding} and \emph{error detection}. For this, we implemented these functions for all three error codes using C++ template-meta programming to let the compiler unroll the code. We always implemented scalar and SSE4.2 variants, and AVX2 for AN and XOR. In the experiments, we processed about 250 Million 16\=bit integers depending on either the blocksize (XOR) or a loop unroll factor (AN, Hamming). The blocksize indicates over how many values a checksum is computed, whereas Hamming and AN coding is applied on each single value. Furthermore, our vectorized algorithms take into account that the number of values may not be aligned to the blocksize/unroll factor. 

\textbf{Hardening Overhead.}
\Cref{fig:codingbenchmark:hardening:seq,fig:codingbenchmark:hardening:vec} show the runtimes for data hardening/encoding. As we can see, Hamming coding is more than an order of magnitude slower than XOR and AN coding. For AN coding, signed and unsigned are equal.

\textbf{Decoding Overhead.}
\Cref{fig:codingbenchmark:decode:seq,fig:codingbenchmark:decode:vec} show that decoding is straightforward for Hamming and XOR, as the redundant bits are easily separable from the original data bits. The original AN coding, however, requires expensive integer division, so that both sequential and vectorized variants are more than an order of magnitude slower than the other two.

\textbf{Detection Overhead.}
For bit flip detection, Hamming and XOR have to recompute the redundant bits and compare them against those retrieved from memory. This is basically the same as encoding with additional comparisons. \Cref{fig:codingbenchmark:check:seq,fig:codingbenchmark:check:vec} show that XOR detection is the fastest. Original AN coding shows poor performance due to the expensive modulo operator, for which no SSE or AVX SIMD instructions exist. Hamming is again more than an order of magnitude slower than XOR, but since population count computation can be vectorized, it comes closer to AN coding.

\textbf{Conclusion.} From a performance perspective, the original AN coding approach does not perform very well compared to XOR checksums. Thus, we introduced our improvements in Section~\ref{sec:ANCodingImprovements}.



\subsection{Evaluation of AN Coding Improvements}

In the second micro benchmark series, we evaluated our AN coding improvements for faster softening and detection. \Cref{fig:refinements:decode:seq:zoom,fig:refinements:decode:vec:zoom} show the decoding performance and \Cref{fig:refinements:check:seq:zoom,fig:refinements:check:vec:zoom} depict the detection performance. Comparing these improvements with XOR, we see great improvements over the original AN coding variant. The improved AN coding is much closer to the XOR performance. For these improvements, we require the multiplicative inverse for a given \(A\). As illustrated in~\Cref{fig:ExtEuclideanSpeed}, the calculation can be done on\=the\=fly as well, since the computation time is in the sub\=microsecond range. There, we varied the code word width (\(|\mathbb{C}|\in\{7, 15,31,63, 127\}\)) and for each the bit width of the \(A\) for which to compute the inverse (\(2\leq|A|\leq|\mathbb{C}|\)). Each curve represents an average over \(10.000 \cdot (|\mathbb{C}|-2)\) computations.

\begin{figure}%
	\centering
	\scriptsize
	\graphicspath{{results/microbenchmarks/ext_euclidean/}}
	\input{results/microbenchmarks/ext_euclidean/TestModuloInverseComputation2}%
	\vspace{-3mm}
	\caption{Runtimes for computing multiplicative inverses.}%
	\label{fig:ExtEuclideanSpeed}%
	\vspace{-4mm}
\end{figure}


\textbf{Conclusion.} The micro benchmarks proof that using the multiplicative inverse makes AN coding well competitive, which is why \emph{Continuous} and \emph{Reencoding} perform so good at SSB. %The inverse need not be stored in meta data and can be computed once per operator. 

%\subsection{Summary}
%Regarding decoding and detection runtime, original AN coding performs poorly against other codings like Hamming and XOR checksums. In contrast, the micro benchmarks proof In contrast to Hamming and checksum, AN coding further allows (i) to easy adjust the detection capabilities using the parameter $A$ and (ii) to directly work on the encoded data representation during query processing, which we efficiently used for on-the-fly error detection. 
