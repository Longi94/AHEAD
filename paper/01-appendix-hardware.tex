\section{Protection Techniques}
\label{sec:RelatedWork_Hardware}

\textbf{Hardware-based protection} can be done on three layers~\cite{DBLP:books/daglib/0037372}: (i) transistor, (ii) circuit, and (iii) architectural. On the \emph{transistor layer}, several techniques have been proposed to harden transistors against radiation events like alpha particles or neutron strikes~\cite{itoh1980single,kohara1990mechanism}. For example, thick polyamide can be used for alpha particle protection~\cite{itoh1980single,kohara1990mechanism}. However, this technique cannot be utilized for neutron strikes~\cite{DBLP:books/daglib/0037372}. In general, techniques at this layer have in common that the protection results in adopted fabrication processes using specialized materials~\cite{itoh1980single,kohara1990mechanism,DBLP:books/daglib/0037372}. Therefore, these techniques are very effective, but they produce (i) substantial overhead in terms of area and cost, and (ii) immense validation and verification costs. 

At the \emph{circuit layer}, redundant circuits and error detection/correc\-tion circuits are prominent examples~\cite{dell1997white,ernst2004razor,Kim:2007:MET:1331699.1331719,DBLP:books/daglib/0037372}. For instance, the RAZOR approach introduces shadow flip flops in the pipeline to recover from errors in logic gates~\cite{ernst2004razor}. Memories and caches are usually protected using error correcting codes (ECC) or parity techniques. Current ECC memories are based on Hamming using a (72,64) code, meaning that 64 bits of data are enhanced with 8 bits of parity allowing single error correction and double error detection. However, this is not sufficient to address multi-bit flips. To tackle multi-bit flips advanced ECC schemes have to be used. Examples are (i) IBM's Chipkill approach, wich computes the parity bits from different memory words and even separate DIMMs instead of physically adjacent bits~\cite{dell1997white}, and (ii) \cite{Kim:2007:MET:1331699.1331719}, which shows that other ECC codes like BCH-codes~\cite{moon2005error} can be realized in hardware to be able to correct e.g., 8-bit flips and detect 9-bit flips for 64 bits of data. However, this increases the number of transistors in hardware and consequently impacts the energy demand, the overhead growing quickly as the code strength is increased~\cite{Kim:2007:MET:1331699.1331719}. Additionally, reading and computing the enhanced ECC bits can be a performance bottleneck during read operations~\cite{Kim:2007:MET:1331699.1331719}. To mitigate \emph{disturbance errors} at this layer, hardware vendors improve inter-cell isolation, but this is challenging due to ever-decreasing feature sizes and higher densities~\cite{DBLP:conf/isca/KimDKFLLWLM14,DBLP:conf/date/Mutlu17}. 

At the \emph{architectural layer}, the protection is based upon the redundant execution either in space (using duplicated hardware units) or in time (using the same hardware multiple times for redundant execution and comparing the results). Dual Modular Redundancy (DMR) and Triple Modular Redundancy (TMR) are traditional approaches. Generally, these techniques lead to an increased power usage which may potentially increase the temperature~\cite{DBLP:books/daglib/0037372}. Increased temperatures lead to higher soft error rate and increased aging~\cite{DBLP:books/daglib/0037372}. To lower these effects, multi-/manycore architectures provide soft error tolerance through the availability of a high number of cores. Idle cores can now be exploited to provide redundancy either at the hardware level (using redundant instructions or redundant threads) or operating system level (using redundant thread processes). For example, the Simultaneous Redundant Threading (SRT) approach~\cite{DBLP:conf/isca/ReinhardtM00} adapts the concept of Simultaneous Multithreading (SMT)~\cite{DBLP:conf/isca/TullsenEL95}. SMT was proposed to improve performance via executing program codes of different applications in a simultaneous multithreaded fashion on multiple functional units inside a given processor. In contrast to that, SRT executes two redundant threads of the same application on multiple functional units and then performs the output comparison. 

To summarize, hardware-based protection techniques are usually very effective, but they also have major drawbacks in terms of (i) high area overhead leading at the same time to more power overhead and (ii) performance penalties. Furthermore, the high verification/validation costs make the reliable hardware design and development very expensive and time consuming~\cite{DBLP:books/daglib/0037372}. To overcome these non-negligible drawbacks, a rich set of software-based techniques has evolved. 

\textbf{Classical software-based protection} techniques are~\cite{goloubeva2006software,DBLP:books/daglib/0037372}: (i) N-version programming, (ii) code redundancy, (iii) control flow checking, and (iv) checkpoint recovery. For instance, N-version programming~\cite{DBLP:journals/tse/Avizienis85} is based on implementing multiple program versions of the same specification which reduces the probability of identical errors occurring in two or more versions. State-of-the-art redundancy-based techniques are Error Detection using Duplicated Instructions (EDDI)~\cite{oh2002error} and Software Implemented Fault Tolerance (SWIFT)~\cite{DBLP:conf/cgo/ReisCVRA05}. Both provide software reliability by duplicating instructions, and inserting comparison and checking instructions. However, these techniques incur significant performance overheads~\cite{oh2002error,DBLP:conf/cgo/ReisCVRA05}. %but various extensions and optimizations have been proposed over the last years~\cite{DBLP:conf/cd/ChenNV16,DBLP:conf/emsoft/DobelHE12, DBLP:journals/taco/ReisCVRAM05,kuvaiskii2016elzar}. 

%Moreover, AN coding has also been used for software-based fault tolerance~\cite{DBLP:conf/hase/HoffmannUDSLS14,DBLP:phd/de/Schiffel2011,ulbrich2012eliminating}. For instance, the work of Schiffel~\cite{DBLP:phd/de/Schiffel2011} allows to encode existing software binaries or to add encoding at compile time, where not all variables' states need to be known in advance. However, in her work she only describes encoding integers of size \(|\mathbb{D}|\in\{1, 8, 16, 32\}\) bits and pointers, where the encoded values are always 64 bits large. Furthermore, protecting processors by AN coding was also suggested in~\cite{forin1989vital}.
