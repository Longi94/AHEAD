\section{Hardware Reliability Concerns}
\label{sec:ProblemEvidence}

Hardware components fabricated with nano-scale transistors face serious reliability issues like soft errors, aging, thermal hot spots, and process variations as a consequence of the aggressive transistor miniaturization~\cite{DBLP:books/daglib/0037372}. These issues arise from multiple sources and they jeopardize the correct application execution~\cite{DBLP:books/daglib/0037372}. The recently published book~\cite{DBLP:books/daglib/0037372} summarizes state-of-the-art protection techniques in all hardware as well as software layers and presents new results of a large research initiative. In their work, the authors primarily target on soft errors, because soft errors are one of the most important reliability issues in the nano-era. Especially, they describe the technical backgrounds, why soft errors increase with decreasing feature sizes and higher densities of transistors~\cite{DBLP:books/daglib/0037372}.

\textbf{Shift towards Multi-Bit Flips.}
The soft errors are caused due to \emph{external influences} like energetic particle strikes, and/or \emph{internal disruptive events} like noise transients at circuit, chip or system level, cross talks, and electromagnetic interference~\cite{DBLP:books/daglib/0037372}. The soft error rate (SER) grows exponentially, because the number of transistors on the die increases exponentially~\cite{ibe2010impact,DBLP:books/daglib/0037372}. Consequently, multi-bit flips become much more frequent~\cite{DBLP:journals/micro/Borkar05,DBLP:conf/dac/HenkelBDGNSTW13,DBLP:books/daglib/0037372} and transistor aging leads to a changing bit flip behavior at run\=time where heat stimulates this effect~\cite{DBLP:conf/dac/HenkelBDGNSTW13}. All hardware components are affected, but memory cells are more susceptible than logic gates~\cite{DBLP:conf/dac/HenkelBDGNSTW13,DBLP:conf/asplos/HwangSS12,Kim:2007:MET:1331699.1331719}. 

For example, memory technologies are already much more vulnerable to electromagnetic interference effects~\cite{DBLP:conf/isca/KimDKFLLWLM14,DBLP:conf/date/Mutlu17} (\emph{disturbance errors}) as well as to data retention problems. A practical solution to tackle interference effects in DRAM is to increase the DRAM refresh rate, so that the probability of inducing disturbance errors before DRAM cells get refreshed is reduced~\cite{DBLP:conf/isca/KimDKFLLWLM14,DBLP:conf/date/Mutlu17}. However, refresh operations waste energy and degrade system performance by interfering with memory accesses~\cite{DBLP:conf/isca/KimDKFLLWLM14,DBLP:conf/date/Mutlu17}. In addition, with increasing DRAM device capacities, the following effects with regard to DRAM refresh arise at the same time as well (\emph{data retention problem})~\cite{Khan:2014:EEM:2637364.2592000,DBLP:conf/dsn/KhanLM16,Liu:2013:ESD:2508148.2485928,DBLP:conf/date/Mutlu17}. First, more refresh operations are necessary to maintain data correctly. Second, smaller DRAM capacitors require lower retention times~\cite{DBLP:conf/date/Mutlu17}. Third, the voltage margins separating data values become smaller, and as a result the same amount of charge loss is more likely to cause multi-bit flips~\cite{Khan:2014:EEM:2637364.2592000,DBLP:conf/dsn/KhanLM16,Liu:2013:ESD:2508148.2485928,DBLP:conf/date/Mutlu17}. Theses effects increase as DRAM capacities increase~\cite{Khan:2014:EEM:2637364.2592000,DBLP:conf/dsn/KhanLM16,Liu:2013:ESD:2508148.2485928,DBLP:conf/date/Mutlu17}.

Furthermore, emerging non-volatile memory technologies like PCM~\cite{DBLP:conf/isca/LeeIMB09}, STT-MRAM~\cite{DBLP:conf/ispass/KultursayKSM13}, and PRAM~\cite{DBLP:journals/pieee/WongLYCWCLCT12} exhibit similar and perhaps even more reliability issues~\cite{Khan:2014:EEM:2637364.2592000,DBLP:conf/dsn/KhanLM16,Liu:2013:ESD:2508148.2485928,DBLP:conf/date/Mutlu17}. For example, PCM are based on multi-level cells (MLC) to store multiple bits in one cell which is achieved by using intermediate resistive states for storing information, in addition to the low and high resistance levels~\cite{DBLP:journals/computers/Mittal17}. A major problem in PCM is that a time-dependent resistance drift can effect that a different value than the originally stored one will be read~\cite{DBLP:journals/computers/Mittal17}. Furthermore, if one cell drifts to the incorrect state, other cells are also highly likely to drift in the near future. Due to this correlation, multi-bit errors changing at run-time are much more likely in MLC PCM~\cite{DBLP:journals/computers/Mittal17}. Moreover, heat produced by writing one PCM cell can alter the value stored in many nearby cells (e.g., up to $11$ cells in a $64$ byteblock~\cite{DBLP:conf/dsn/JiangZY14})~\cite{DBLP:journals/computers/Mittal17}.

\textbf{Protection Techniques.}
Hardware components usually feature protection techniques such as error correction codes (ECC)~\cite{hamming1950error,Kim:2007:MET:1331699.1331719} or hardware redundancy~\cite{ernst2004razor,su1980hardware}. However, these approaches are aligned to single bit flips nowadays. Thus, hardware research in two directions is necessary: (i) developing appropriate protection technique to cover new effects like \emph{disturbance errors} and/or (ii) scaling up traditional techniques to cover multi\=bit flips. Research is currently done in both directions~\cite{DBLP:books/daglib/0037372}. For example, to mitigate \emph{disturbance errors}, the inter-cell isolation is improved, but this is challenging due to ever-decreasing feature sizes and higher densities~\cite{DBLP:conf/isca/KimDKFLLWLM14,DBLP:conf/date/Mutlu17}. Furthermore, the implementation of multi\=bit error detection and correction codes in memory has been investigated~\cite{Kim:2007:MET:1331699.1331719}. However, this results in high chip area costs for storing the extra parity bits and increased calculation/checking/correction logic, whereas the overhead quickly grows with the code strength~\cite{Kim:2007:MET:1331699.1331719}. Then, reading and computing the parity bits can be a bottleneck~\cite{Kim:2007:MET:1331699.1331719}.

There are a lot of hardware-oriented activities (see also Appendix~\ref{sec:RelatedWork_Hardware}), but these activities show that hardware-based approaches are very effective, but the protection is very challenging and each technique introduces large performance, chip area, and power overheads. Furthermore, the techniques have to be implemented in a \emph{pessimistic} way to cover the \emph{aging} aspect leading usually to an over-provisioning. The whole is made more difficult by \emph{Dark Silicon}~\cite{DBLP:journals/micro/EsmaeilzadehBASB12}: billions of transistors can be put on a chip, but not all them can be used at the same time. This and the various new disruptive interference effects make the reliable hardware design and development very challenging, time consuming, and very expensive~\cite{DBLP:books/daglib/0037372}. The disadvantages outweigh the advantages for hardware-based protection, so that the semiconductor as well as hardware/software communities have recently experienced a shift towards mitigating these reliability issues also at higher software layers, rather than completely mitigating these issues only in hardware~\cite{DBLP:conf/dac/HenkelBDGNSTW13,DBLP:books/daglib/0037372,DBLP:journals/it/ShafiqueABCCDEH15}. 

\textbf{Consequences for Database Systems.}
Since multi-bit flips will occur more frequently in future hardware and are not handled at the hardware layer, a major challenge is resilient query processing~\cite{DBLP:journals/pvldb/BohmLF11}. To tackle that, an emerging research direction will be employing protection techniques in database systems and using the available knowledge to specialize as well as to balance protection and the associated overhead. That means, appropriate solutions have to satisfy the following requirements:  
\begin{compactenum}
\item[\textbf{R1:}] Solutions have to \emph{detect} and later correct (i) errors (multi\=bit flips) that modify data stored in main memory, (ii) errors induced during transferring on interconnects and (iii) errors induced during computations during query processing (\emph{detection capability}).
\item[\textbf{R2:}] Solutions should be adaptable at run-time for different error models because the number and the rate of bit flips may vary over hardware generations or due to hardware aging effects. (\emph{run-time adaptability}).
\item[\textbf{R3:}] Solutions should only introduce the necessary overhead in terms of memory consumption and query runtime, which is required to detect a desired number of bit flips. That means the overhead should be as small as possible, but still provide a reliable behavior (\emph{balanced overhead}). 
\end{compactenum}
In the remainder of this paper, we focus on \textbf{error detection} as a first step and present a novel approach. Our approach is tailored to state-of-the-art in-memory column stores and satisfies \textbf{R1} to \textbf{R3}. %As we will show, our \emph{AHEAD} approach satisfies all three requirements. 
